{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q6r_mvwuVRbK"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Irz39yQORlH0"
      },
      "outputs": [],
      "source": [
        "# data\n",
        "dataset_name = \"oxford_flowers102\"\n",
        "dataset_repetitions = 5\n",
        "num_epochs = 1  # train for at least 50 epochs for good results\n",
        "image_size = 64\n",
        "# KID = Kernel Inception Distance, see related section\n",
        "kid_image_size = 75\n",
        "kid_diffusion_steps = 5\n",
        "plot_diffusion_steps = 20\n",
        "\n",
        "# sampling\n",
        "min_signal_rate = 0.02\n",
        "max_signal_rate = 0.95\n",
        "\n",
        "# architecture\n",
        "embedding_dims = 32\n",
        "embedding_max_frequency = 1000.0\n",
        "widths = [32, 64, 96, 128]\n",
        "block_depth = 2\n",
        "\n",
        "# optimization\n",
        "batch_size = 64\n",
        "ema = 0.999\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "clm_ibGDQL59"
      },
      "outputs": [],
      "source": [
        "class SinusoidalEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SinusoidalEmbedding, self).__init__()\n",
        "        self.embedding_min_frequency = 1.0 #it is defined in the keras\n",
        "        self.embedding_dims = 32 # it is defined as half of the image's dimension, stays as hyperparameter \n",
        "        self.embedding_max_frequency = 1000.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        frequencies = torch.exp(\n",
        "            torch.linspace( \n",
        "                torch.log(torch.tensor(self.embedding_min_frequency)),\n",
        "                torch.log(torch.tensor(self.embedding_max_frequency)),\n",
        "                self.embedding_dims // 2,\n",
        "            )\n",
        "        )\n",
        "        angular_speeds = 2.0 * math.pi * frequencies\n",
        "         \n",
        "        angular= angular_speeds * x\n",
        "        freq_shapes = angular.shape\n",
        "        angular = angular.view(freq_shapes[0],freq_shapes[-1],freq_shapes[1],freq_shapes[2])\n",
        "        embeddings = torch.cat(\n",
        "            (torch.sin(angular), torch.cos(angular)), dim=1)\n",
        "         \n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n1geJC_2QMAT"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, width,fi):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "   \n",
        "        self.width = width\n",
        "        self.activation = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, padding=1)\n",
        "        self.batch = nn.BatchNorm2d(fi, affine=False)\n",
        "        self.conv3 = nn.Conv2d(fi, width, kernel_size=3, padding=1)\n",
        "        self.conv0 =  nn.Conv2d(fi,  width, kernel_size=3, padding=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        input_width = x.shape[1]\n",
        "        if input_width == self.width:\n",
        "            residual = x\n",
        "        else:\n",
        "            residual =  self.conv0(x)\n",
        "\n",
        "        x = self.batch(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x)\n",
        "        x += residual\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QLpGKmj9QMD2"
      },
      "outputs": [],
      "source": [
        "class DownBlock(nn.Module):\n",
        "  def __init__(self,width,fi):\n",
        "      super(DownBlock, self).__init__()\n",
        "      self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
        "    \n",
        "      self.resBlock = ResidualBlock( width,fi)\n",
        "      self.resBlock2 = ResidualBlock( width,width)\n",
        "        \n",
        "  def forward(self, x):\n",
        "      x, skips = x\n",
        "      x = self.resBlock(x)\n",
        "      skips.append(x)\n",
        "      x = self.resBlock2(x)\n",
        "      skips.append(x)\n",
        "      x = self.avgpool(x)\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YuSChgD8VM-_"
      },
      "outputs": [],
      "source": [
        "class UpBlock(nn.Module):\n",
        "  def __init__(self, width,fi):\n",
        "      super(UpBlock, self).__init__()\n",
        "      self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "       \n",
        "      self.resBlock = ResidualBlock( width,fi + width)\n",
        "      self.resBlock2 = ResidualBlock(width,width*2)\n",
        "  def forward(self, x):\n",
        "      x, skips = x\n",
        "      x= self.upsample(x) \n",
        "      x = torch.cat([x, skips.pop()], dim=1)\n",
        "     \n",
        "      x =self.resBlock(x)\n",
        "      x = torch.cat([x, skips.pop()], dim=1)\n",
        "      \n",
        "      x =self.resBlock2(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E5nJ5odNWfxq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ResidualUNet(nn.Module):\n",
        "    def __init__(self, image_size, widths, block_depth):\n",
        "        super(ResidualUNet, self).__init__()\n",
        "        self.conv = nn.Conv2d(widths[0],widths[0], kernel_size=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(widths[0],3, kernel_size=1, bias=False)\n",
        "        \n",
        "        self.downBlock0 = DownBlock(32,64)\n",
        "        self.downBlock1 = DownBlock(64,32)\n",
        "        self.downBlock2 = DownBlock(96,64)\n",
        "   \n",
        "\n",
        "\n",
        "        self.upBlock0 = UpBlock(96,128) \n",
        "        self.upBlock1 = UpBlock(64,96) \n",
        "        self.upBlock2 = UpBlock(32,64) \n",
        "        \n",
        "        \n",
        "     \n",
        "        \n",
        "        self.resBlock0 = ResidualBlock(128,96)\n",
        "        self.resBlock1 = ResidualBlock(128,128)\n",
        "\n",
        "        self.sin = SinusoidalEmbedding()\n",
        "    def forward(self, noisy_images, noise_variances):\n",
        "\n",
        " \n",
        "        e =  self.sin(noise_variances)\n",
        "        \n",
        "        e = F.interpolate(e, size=image_size, mode='nearest')\n",
        "         \n",
        "        x = self.conv(e)\n",
        "        x = torch.cat([x, e], dim=1)\n",
        "        skips = []\n",
        "\n",
        "        x = self.downBlock0([x, skips])\n",
        "      \n",
        "        x = self.downBlock1([x, skips])\n",
        "    \n",
        "        x = self.downBlock2([x, skips])\n",
        " \n",
        "     \n",
        "        \n",
        "        x = self.resBlock0(x)\n",
        "         \n",
        "        x = self.resBlock1(x)\n",
        "      \n",
        "  \n",
        "        x = self.upBlock0([x, skips])\n",
        "        x = self.upBlock1([x, skips])\n",
        "        x = self.upBlock2([x, skips])\n",
        "            \n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.utils.data import  ConcatDataset\n",
        "\n",
        "\n",
        "from  torch.utils.data.sampler import SubsetRandomSampler  \n",
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(500),\n",
        "    transforms.Resize(size=(image_size, image_size), interpolation=Image.LANCZOS),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: torch.clamp(x/255.0, min=0.0, max=1.0))\n",
        "     \n",
        "])\n",
        "# Define the data transformation pi\n",
        "# Load the dataset\n",
        "dataset = datasets.Flowers102(root='./data', split='test', download=True, transform=transform)\n",
        "dataset1 = datasets.Flowers102(root='./data', split='train', download=True, transform=transform)\n",
        "dataset2 = datasets.Flowers102(root='./data', split='val', download=True, transform=transform)\n",
        "\n",
        "concat_dataset = ConcatDataset([dataset,dataset1,dataset2,dataset,dataset1,dataset2,dataset,dataset1,dataset2,dataset,dataset1,dataset2,dataset,dataset1,dataset2] ) \n",
        "\n",
        "\n",
        "data_loader = DataLoader(concat_dataset, batch_size=batch_size  ,      shuffle=True, drop_last=True )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YxCLzepDk0zq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(concat_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w58j4zK5J8m-",
        "outputId": "84de7394-9dea-4e85-fdc4-527aabf5fc03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40945"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5YrqubJ6jeXU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, image_size, widths, block_depth):\n",
        "        super().__init__()\n",
        "        self.network = ResidualUNet( image_size, widths, block_depth)\n",
        "        self.ema_network = ResidualUNet( image_size, widths, block_depth)\n",
        "        self.ema_decay = 0.99\n",
        "\n",
        " \n",
        "    def denormalize(self, images):\n",
        "        # convert the pixel values back to 0-1 range\n",
        "        images = nn.BatchNorm2d(num_features=images.shape[1])(images).mean() + images *  torch.var(nn.BatchNorm2d(num_features=images.shape[1])(images))**0.5\n",
        "        return torch.clamp(images, 0.0, 1.0)\n",
        "    \n",
        "    def diffusion_schedule(self, diffusion_times):\n",
        "        # diffusion times -> angles\n",
        "        start_angle = torch.acos(torch.tensor(max_signal_rate))\n",
        "        end_angle = torch.acos(torch.tensor(min_signal_rate))\n",
        "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
        "        # angles -> signal and noise rates\n",
        "        signal_rates = torch.cos(diffusion_angles)\n",
        "        noise_rates = torch.sin(diffusion_angles)\n",
        "        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n",
        "        # the exponential moving average weights are used at evaluation\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "\n",
        "\n",
        "        # predict noise component and calculate the image component using it\n",
        "        pred_noises = network(noisy_images, noise_rates.pow(2))\n",
        "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
        "        return pred_noises, pred_images\n",
        "\n",
        "\n",
        "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
        "        # reverse diffusion = sampling\n",
        "        num_images = initial_noise.shape[0]\n",
        "        step_size = 1.0 / diffusion_steps\n",
        "\n",
        "        # important line:\n",
        "        # at the first sampling step, the \"noisy image\" is pure noise\n",
        "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
        "        next_noisy_images = initial_noise\n",
        "        for step in range(diffusion_steps):\n",
        "            noisy_images = next_noisy_images\n",
        "\n",
        "            # separate the current noisy image to its components\n",
        "            diffusion_times = torch.ones((num_images, 1, 1, 1)) - step * step_size\n",
        "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "            pred_noises, pred_images = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=True\n",
        "            )\n",
        "            # network used in eval mode\n",
        "\n",
        "            # remix the predicted components using the next signal and noise rates\n",
        "            next_diffusion_times = diffusion_times - step_size\n",
        "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
        "                next_diffusion_times\n",
        "            )\n",
        "            next_noisy_images = (                next_signal_rates * pred_images + next_noise_rates * pred_noises )\n",
        "            # this new noisy image will be used in the next step\n",
        "\n",
        "        return pred_images\n",
        "\n",
        "    def generate(self, num_images, diffusion_steps):\n",
        "        # noise -> images -> denormalized images\n",
        "        initial_noise = torch.randn(num_images, 3, image_size, image_size)\n",
        "        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
        "        generated_images = self.denormalize(generated_images)\n",
        "        return generated_images\n",
        "\n",
        "    def ema_para(self,network_parameters):\n",
        "      for param, ema_param in zip(network_parameters,self.ema_network.parameters()):\n",
        "        ema_param.data = self.ema_decay * ema_param.data + (1 -  self.ema_decay) * param.data\n",
        "\n",
        "    def forward(self, images):\n",
        "        # normalize images to have standard deviation of 1, like the noises\n",
        "        images = nn.BatchNorm2d(num_features=images.shape[1])(images)\n",
        "        noises = torch.randn(images.shape[0],3,image_size, image_size)\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = torch.rand(images.shape[0], 1, 1, 1)\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)       \n",
        "        noisy_images = signal_rates * images + noise_rates * noises\n",
        "\n",
        "        pred_noises, pred_images = self.denoise( noisy_images, noise_rates, signal_rates, training=True)\n",
        "        #self.ema_para(self.network.parameters())\n",
        "        return pred_noises, noises\n",
        "\n",
        "    def test_step(self, images ):\n",
        "        images = nn.BatchNorm2d(num_features=images.shape[1])(images)\n",
        "        noises = torch.randn(images.shape[0],3,image_size, image_size)\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = torch.rand(images.shape[0], 1, 1, 1)\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)       \n",
        "        noisy_images = signal_rates * images + noise_rates * noises\n",
        "\n",
        "        pred_noises, pred_images = self.denoise( noisy_images, noise_rates, signal_rates, training=True)\n",
        "\n",
        "        images = self.denormalize(images)\n",
        "        generated_images = self.generate( num_images=batch_size, diffusion_steps=kid_diffusion_steps    )\n",
        "        return generated_images\n",
        "\n",
        "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n",
        "            # plot random generated images for visual evaluation of generation quality\n",
        "        generated_images = self.generate(   num_images=num_rows * num_cols,    diffusion_steps=plot_diffusion_steps     )\n",
        "\n",
        "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
        "        for row in range(num_rows):\n",
        "            for col in range(num_cols):\n",
        "                index = row * num_cols + col\n",
        "                plt.subplot(num_rows, num_cols, index + 1)\n",
        "                plt.imshow(generated_images[index].permute(1, 2, 0).detach().numpy())\n",
        "                plt.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diffusion_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGCK6HInSEGu",
        "outputId": "c0761822-e63d-4e0d-e996-8a4e6ee3a308"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of DiffusionModel(\n",
              "  (network): ResidualUNet(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (downBlock0): DownBlock(\n",
              "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (downBlock1): DownBlock(\n",
              "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (downBlock2): DownBlock(\n",
              "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (upBlock0): UpBlock(\n",
              "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(224, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(224, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (upBlock1): UpBlock(\n",
              "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(160, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(160, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (upBlock2): UpBlock(\n",
              "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (resBlock0): ResidualBlock(\n",
              "      (activation): SiLU()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "      (conv3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv0): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (resBlock1): ResidualBlock(\n",
              "      (activation): SiLU()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (sin): SinusoidalEmbedding()\n",
              "  )\n",
              "  (ema_network): ResidualUNet(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (downBlock0): DownBlock(\n",
              "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (downBlock1): DownBlock(\n",
              "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (downBlock2): DownBlock(\n",
              "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (upBlock0): UpBlock(\n",
              "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(224, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(224, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (upBlock1): UpBlock(\n",
              "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(160, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(160, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (upBlock2): UpBlock(\n",
              "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "      (resBlock): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (resBlock2): ResidualBlock(\n",
              "        (activation): SiLU()\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (resBlock0): ResidualBlock(\n",
              "      (activation): SiLU()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "      (conv3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv0): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (resBlock1): ResidualBlock(\n",
              "      (activation): SiLU()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (sin): SinusoidalEmbedding()\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZXQPglC-jeaY"
      },
      "outputs": [],
      "source": [
        "diffusion_model  = DiffusionModel(image_size, widths, block_depth)\n",
        "\n",
        " \n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=diffusion_model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "loss_fn = nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(data_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        images, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        pred_noises, noises = diffusion_model(images)\n",
        " \n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(pred_noises, noises)\n",
        "        print(i,loss)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "         \n",
        "        \n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 16 == 15:\n",
        "            last_loss = running_loss / 16 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(data_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "             \n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WM41uwljeeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95feacd7-51e7-4781-b52b-2dd9ce5d448d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "0 tensor(0.8052, grad_fn=<MeanBackward0>)\n",
            "1 tensor(0.8165, grad_fn=<MeanBackward0>)\n",
            "2 tensor(0.8131, grad_fn=<MeanBackward0>)\n",
            "3 tensor(0.8201, grad_fn=<MeanBackward0>)\n",
            "4 tensor(0.8795, grad_fn=<MeanBackward0>)\n",
            "5 tensor(0.8309, grad_fn=<MeanBackward0>)\n",
            "6 tensor(0.8483, grad_fn=<MeanBackward0>)\n",
            "7 tensor(0.8022, grad_fn=<MeanBackward0>)\n",
            "8 tensor(0.8035, grad_fn=<MeanBackward0>)\n",
            "9 tensor(0.8028, grad_fn=<MeanBackward0>)\n",
            "10 tensor(0.8083, grad_fn=<MeanBackward0>)\n",
            "11 tensor(0.8094, grad_fn=<MeanBackward0>)\n",
            "12 tensor(0.8023, grad_fn=<MeanBackward0>)\n",
            "13 tensor(0.8022, grad_fn=<MeanBackward0>)\n",
            "14 tensor(0.8011, grad_fn=<MeanBackward0>)\n",
            "15 tensor(0.8002, grad_fn=<MeanBackward0>)\n",
            "  batch 16 loss: 0.8153457418084145\n",
            "16 tensor(0.8022, grad_fn=<MeanBackward0>)\n",
            "17 tensor(0.8096, grad_fn=<MeanBackward0>)\n",
            "18 tensor(0.8181, grad_fn=<MeanBackward0>)\n",
            "19 tensor(0.8242, grad_fn=<MeanBackward0>)\n",
            "20 tensor(0.8028, grad_fn=<MeanBackward0>)\n",
            "21 tensor(0.8031, grad_fn=<MeanBackward0>)\n",
            "22 tensor(0.8315, grad_fn=<MeanBackward0>)\n",
            "23 tensor(0.8237, grad_fn=<MeanBackward0>)\n",
            "24 tensor(0.8163, grad_fn=<MeanBackward0>)\n",
            "25 tensor(0.8307, grad_fn=<MeanBackward0>)\n",
            "26 tensor(0.8066, grad_fn=<MeanBackward0>)\n",
            "27 tensor(0.8206, grad_fn=<MeanBackward0>)\n",
            "28 tensor(0.8191, grad_fn=<MeanBackward0>)\n",
            "29 tensor(0.8120, grad_fn=<MeanBackward0>)\n",
            "30 tensor(0.8016, grad_fn=<MeanBackward0>)\n",
            "31 tensor(0.8014, grad_fn=<MeanBackward0>)\n",
            "  batch 32 loss: 1.6293096765875816\n",
            "32 tensor(0.8016, grad_fn=<MeanBackward0>)\n",
            "33 tensor(0.8010, grad_fn=<MeanBackward0>)\n",
            "34 tensor(0.7998, grad_fn=<MeanBackward0>)\n",
            "35 tensor(0.8011, grad_fn=<MeanBackward0>)\n",
            "36 tensor(0.8015, grad_fn=<MeanBackward0>)\n",
            "37 tensor(0.8109, grad_fn=<MeanBackward0>)\n",
            "38 tensor(0.8318, grad_fn=<MeanBackward0>)\n",
            "39 tensor(0.8016, grad_fn=<MeanBackward0>)\n",
            "40 tensor(0.8023, grad_fn=<MeanBackward0>)\n",
            "41 tensor(0.8015, grad_fn=<MeanBackward0>)\n",
            "42 tensor(0.8059, grad_fn=<MeanBackward0>)\n",
            "43 tensor(0.8030, grad_fn=<MeanBackward0>)\n",
            "44 tensor(0.8030, grad_fn=<MeanBackward0>)\n",
            "45 tensor(0.8018, grad_fn=<MeanBackward0>)\n",
            "46 tensor(0.8020, grad_fn=<MeanBackward0>)\n",
            "47 tensor(0.8048, grad_fn=<MeanBackward0>)\n",
            "  batch 48 loss: 2.4339072592556477\n",
            "48 tensor(0.8038, grad_fn=<MeanBackward0>)\n",
            "49 tensor(0.8039, grad_fn=<MeanBackward0>)\n",
            "50 tensor(0.7998, grad_fn=<MeanBackward0>)\n",
            "51 tensor(0.8007, grad_fn=<MeanBackward0>)\n",
            "52 tensor(0.8006, grad_fn=<MeanBackward0>)\n",
            "53 tensor(0.8009, grad_fn=<MeanBackward0>)\n",
            "54 tensor(0.8009, grad_fn=<MeanBackward0>)\n",
            "55 tensor(0.7995, grad_fn=<MeanBackward0>)\n",
            "56 tensor(0.7986, grad_fn=<MeanBackward0>)\n",
            "57 tensor(0.7997, grad_fn=<MeanBackward0>)\n",
            "58 tensor(0.7990, grad_fn=<MeanBackward0>)\n",
            "59 tensor(0.7994, grad_fn=<MeanBackward0>)\n",
            "60 tensor(0.7990, grad_fn=<MeanBackward0>)\n",
            "61 tensor(0.7988, grad_fn=<MeanBackward0>)\n",
            "62 tensor(0.7988, grad_fn=<MeanBackward0>)\n",
            "63 tensor(0.7999, grad_fn=<MeanBackward0>)\n",
            "  batch 64 loss: 3.2341172359883785\n",
            "64 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "65 tensor(0.7990, grad_fn=<MeanBackward0>)\n",
            "66 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "67 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "68 tensor(0.7991, grad_fn=<MeanBackward0>)\n",
            "69 tensor(0.7991, grad_fn=<MeanBackward0>)\n",
            "70 tensor(0.7989, grad_fn=<MeanBackward0>)\n",
            "71 tensor(0.7976, grad_fn=<MeanBackward0>)\n",
            "72 tensor(0.7981, grad_fn=<MeanBackward0>)\n",
            "73 tensor(0.7972, grad_fn=<MeanBackward0>)\n",
            "74 tensor(0.7986, grad_fn=<MeanBackward0>)\n",
            "75 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "76 tensor(0.7977, grad_fn=<MeanBackward0>)\n",
            "77 tensor(0.7987, grad_fn=<MeanBackward0>)\n",
            "78 tensor(0.7987, grad_fn=<MeanBackward0>)\n",
            "79 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "  batch 80 loss: 4.032350484281778\n",
            "80 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "81 tensor(0.7990, grad_fn=<MeanBackward0>)\n",
            "82 tensor(0.7973, grad_fn=<MeanBackward0>)\n",
            "83 tensor(0.7985, grad_fn=<MeanBackward0>)\n",
            "84 tensor(0.7986, grad_fn=<MeanBackward0>)\n",
            "85 tensor(0.7977, grad_fn=<MeanBackward0>)\n",
            "86 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "87 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "88 tensor(0.7985, grad_fn=<MeanBackward0>)\n",
            "89 tensor(0.7973, grad_fn=<MeanBackward0>)\n",
            "90 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "91 tensor(0.7972, grad_fn=<MeanBackward0>)\n",
            "92 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "93 tensor(0.7974, grad_fn=<MeanBackward0>)\n",
            "94 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "95 tensor(0.7988, grad_fn=<MeanBackward0>)\n",
            "  batch 96 loss: 4.830388717353344\n",
            "96 tensor(0.7976, grad_fn=<MeanBackward0>)\n",
            "97 tensor(0.7986, grad_fn=<MeanBackward0>)\n",
            "98 tensor(0.7976, grad_fn=<MeanBackward0>)\n",
            "99 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "100 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "101 tensor(0.7966, grad_fn=<MeanBackward0>)\n",
            "102 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "103 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "104 tensor(0.7966, grad_fn=<MeanBackward0>)\n",
            "105 tensor(0.7977, grad_fn=<MeanBackward0>)\n",
            "106 tensor(0.7992, grad_fn=<MeanBackward0>)\n",
            "107 tensor(0.7996, grad_fn=<MeanBackward0>)\n",
            "108 tensor(0.7967, grad_fn=<MeanBackward0>)\n",
            "109 tensor(0.7971, grad_fn=<MeanBackward0>)\n",
            "110 tensor(0.7977, grad_fn=<MeanBackward0>)\n",
            "111 tensor(0.7991, grad_fn=<MeanBackward0>)\n",
            "  batch 112 loss: 5.628244947642088\n",
            "112 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "113 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "114 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "115 tensor(0.7984, grad_fn=<MeanBackward0>)\n",
            "116 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "117 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "118 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "119 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "120 tensor(0.7965, grad_fn=<MeanBackward0>)\n",
            "121 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "122 tensor(0.7974, grad_fn=<MeanBackward0>)\n",
            "123 tensor(0.7984, grad_fn=<MeanBackward0>)\n",
            "124 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "125 tensor(0.7995, grad_fn=<MeanBackward0>)\n",
            "126 tensor(0.7954, grad_fn=<MeanBackward0>)\n",
            "127 tensor(0.7973, grad_fn=<MeanBackward0>)\n",
            "  batch 128 loss: 6.4259638376533985\n",
            "128 tensor(0.7985, grad_fn=<MeanBackward0>)\n",
            "129 tensor(0.7968, grad_fn=<MeanBackward0>)\n",
            "130 tensor(0.7970, grad_fn=<MeanBackward0>)\n",
            "131 tensor(0.7976, grad_fn=<MeanBackward0>)\n",
            "132 tensor(0.7973, grad_fn=<MeanBackward0>)\n",
            "133 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "134 tensor(0.7970, grad_fn=<MeanBackward0>)\n",
            "135 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "136 tensor(0.7973, grad_fn=<MeanBackward0>)\n",
            "137 tensor(0.7976, grad_fn=<MeanBackward0>)\n",
            "138 tensor(0.7972, grad_fn=<MeanBackward0>)\n",
            "139 tensor(0.7974, grad_fn=<MeanBackward0>)\n",
            "140 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "141 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "142 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "143 tensor(0.7985, grad_fn=<MeanBackward0>)\n",
            "  batch 144 loss: 7.223622158169746\n",
            "144 tensor(0.7971, grad_fn=<MeanBackward0>)\n",
            "145 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "146 tensor(0.7971, grad_fn=<MeanBackward0>)\n",
            "147 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "148 tensor(0.7967, grad_fn=<MeanBackward0>)\n",
            "149 tensor(0.7986, grad_fn=<MeanBackward0>)\n",
            "150 tensor(0.7984, grad_fn=<MeanBackward0>)\n",
            "151 tensor(0.7969, grad_fn=<MeanBackward0>)\n",
            "152 tensor(0.7971, grad_fn=<MeanBackward0>)\n",
            "153 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "154 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "155 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "156 tensor(0.7971, grad_fn=<MeanBackward0>)\n",
            "157 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "158 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "159 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "  batch 160 loss: 8.02122138813138\n",
            "160 tensor(0.7968, grad_fn=<MeanBackward0>)\n",
            "161 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "162 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "163 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "164 tensor(0.7977, grad_fn=<MeanBackward0>)\n",
            "165 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "166 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "167 tensor(0.7974, grad_fn=<MeanBackward0>)\n",
            "168 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "169 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "170 tensor(0.7981, grad_fn=<MeanBackward0>)\n",
            "171 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "172 tensor(0.7981, grad_fn=<MeanBackward0>)\n",
            "173 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "174 tensor(0.7976, grad_fn=<MeanBackward0>)\n",
            "175 tensor(0.7979, grad_fn=<MeanBackward0>)\n",
            "  batch 176 loss: 8.819031581282616\n",
            "176 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "177 tensor(0.7988, grad_fn=<MeanBackward0>)\n",
            "178 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "179 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "180 tensor(0.7966, grad_fn=<MeanBackward0>)\n",
            "181 tensor(0.7990, grad_fn=<MeanBackward0>)\n",
            "182 tensor(0.7976, grad_fn=<MeanBackward0>)\n",
            "183 tensor(0.7970, grad_fn=<MeanBackward0>)\n",
            "184 tensor(0.7980, grad_fn=<MeanBackward0>)\n",
            "185 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "186 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "187 tensor(0.7989, grad_fn=<MeanBackward0>)\n",
            "188 tensor(0.7981, grad_fn=<MeanBackward0>)\n",
            "189 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "190 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "191 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "  batch 192 loss: 9.616957120597363\n",
            "192 tensor(0.7973, grad_fn=<MeanBackward0>)\n",
            "193 tensor(0.7978, grad_fn=<MeanBackward0>)\n",
            "194 tensor(0.7985, grad_fn=<MeanBackward0>)\n",
            "195 tensor(0.7975, grad_fn=<MeanBackward0>)\n",
            "196 tensor(0.7972, grad_fn=<MeanBackward0>)\n",
            "197 tensor(0.7977, grad_fn=<MeanBackward0>)\n",
            "198 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "199 tensor(0.7981, grad_fn=<MeanBackward0>)\n",
            "200 tensor(0.7989, grad_fn=<MeanBackward0>)\n",
            "201 tensor(0.7987, grad_fn=<MeanBackward0>)\n",
            "202 tensor(0.7983, grad_fn=<MeanBackward0>)\n",
            "203 tensor(0.7977, grad_fn=<MeanBackward0>)\n",
            "204 tensor(0.7982, grad_fn=<MeanBackward0>)\n",
            "205 tensor(0.7985, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 3\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "diffusion_model.train(True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    \n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "    \n",
        "\n",
        "\n",
        "    epoch_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "diffusion_model.plot_images()"
      ],
      "metadata": {
        "id": "QuOVmWqBvERp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in data_loader:\n",
        "  img, lbl = data\n",
        "  print(img)"
      ],
      "metadata": {
        "id": "Pk1xBxNTtesi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in data_loader:\n",
        "  img, lbl = data\n",
        "  print(img)"
      ],
      "metadata": {
        "id": "wcd1OxB29UgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yk5Y_loi9XqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}